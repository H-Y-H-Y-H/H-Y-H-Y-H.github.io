---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>


# ğŸ”¥ News
- *2024.02*: &nbsp;ğŸ‰ğŸ‰ Paper accepted: Science Robotics (Release on March. 27, 2024). 

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> arxiv </div><img src='images/knolling.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations**

ğŸ¤– Attention busy people! Imagine you are very busy and don't have time to tell the robot where everything should be placed!ğŸ§¹ So, don't you think robots need to understand tidiness? ğŸ¤”

This paperğŸ“‘ shows how robots can learn the concept of ["knolling"](https://medium.com/@Lyst/your-favorite-insta-aesthetic-has-an-unexpected-history-a929e078d4ea) from tidy demonstrations, allowing them to automatically organize your table in a neat and efficient way. ğŸ§¹âœ¨


**Yuhang Hu**, Zhizhuo Zhang, Xinyue Zhu, Ruibo Liu, Philippe Wyder, Hod Lipson 

ğŸ¬[VIDEO](https://youtu.be/jCxykR4iP0I)      ğŸ“„[PAPER](https://arxiv.org/pdf/2310.04566.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/hideandseek.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Egocentric Visual Self-Modeling for Autonomous Robot Dynamics Prediction and Adaptation**

Humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict othersâ€™ plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). 

Boyuan Chen, **Yuhang Hu**, Robert Kwiatkowski, Shuran Song, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/xOv4nO4WWQQ)      ğŸ“„[PAPER](https://arxiv.org/pdf/2207.03386.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023 @ Robot Learning Workshop</div><img src='images/knolling2.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**Enhancing Object Organization with Self-supervised Graspability Estimation**

A robotic system that integrates visual perception, a self-supervised graspability estimation model, knolling models, and robot arm controllers to efficiently organize an arbitrary number of objects on a table, even when they are closely clustered or stacked.

**Yuhang Hu**, Zhizhuo Zhang, Hod Lipson 

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=_pskiKonX38&t=12s)      ğŸ“„[PAPER](https://arxiv.org/pdf/2310.19226.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/eva.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Smile like you mean it: Driving animatronic robotic face with learned models**

Eva 2.0: A physical humanoid face robot with soft skin â• A vision-based self-supervised learning framework for facial mimicry.

Boyuan Chen, **Yuhang Hu**, Lianfeng Li, Sara Cummings, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/1vBLI-q04kM?si=gDofkitYstuisSRh)      ğŸ“„[PAPER](https://arxiv.org/pdf/2105.12724.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/hideandseek.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Visual perspective taking for opponent behavior modeling**

Humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict othersâ€™ plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). 

Boyuan Chen, **Yuhang Hu**, Robert Kwiatkowski, Shuran Song, Hod Lipson

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=gDtZoU3ayhg)      ğŸ“„[PAPER](https://arxiv.org/pdf/2105.05145.pdf)


