---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>


# ğŸ”¥ News
- *2024.02*: &nbsp;ğŸ‰ğŸ‰ Paper accepted: Science Robotics (Release on March. 27, 2024). 

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Science Robotics </div><img src='images/emo_coexp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**Human-robot facial coexpression**

Humanoid robots are capable of mimicking human expressions by perceiving human emotions and responding after the human has finished their expression. However, a delayed smile can feel artificial and disingenuous compared with a smile occurring simultaneously with a companionâ€™s smile. We trained our anthropomorphic facial robot named Emo to display an anticipatory expression to match its human companion. Emo is equipped with 26 motors and flexible silicone skin to provide precise control over its facial expressions. The robot was trained with a video dataset of humans making expressions. By observing subtle changes in a human face, the robot could predict an approaching smile 839 milliseconds before the human smiled and adjust its face to smile simultaneously.


**Yuhang Hu**, Boyuan Chen, Jiong Lin, Yunzhe Wang, Yingke Wang, Cameron Mehlman, Hod Lipson 

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=pWTTzR_wXuQ)      ğŸ“„[PAPER](https://www.science.org/doi/10.1126/scirobotics.adi4724)

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> arxiv </div><img src='images/knolling.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations**

ğŸ¤– Attention busy people! Imagine you are very busy and don't have time to tell the robot where everything should be placed!ğŸ§¹ So, don't you think robots need to understand tidiness? ğŸ¤”

This paperğŸ“‘ shows how robots can learn the concept of ["knolling"](https://medium.com/@Lyst/your-favorite-insta-aesthetic-has-an-unexpected-history-a929e078d4ea) from tidy demonstrations, allowing them to automatically organize your table in a neat and efficient way. ğŸ§¹âœ¨


**Yuhang Hu**, Zhizhuo Zhang, Xinyue Zhu, Ruibo Liu, Philippe Wyder, Hod Lipson 

ğŸ¬[VIDEO](https://youtu.be/jCxykR4iP0I)      ğŸ“„[PAPER](https://arxiv.org/pdf/2310.04566.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/ego.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Egocentric Visual Self-Modeling for Autonomous Robot Dynamics Prediction and Adaptation**

We developed an approach that allows robots to learn their own dynamics using only a first-person camera view, without any prior knowledge! ğŸ¥ğŸ’¡

ğŸ¦¿ Tested on a 12-DoF robot, the self-supervised model showcased the capabilities of basic locomotion tasks.

ğŸ”§ The robot can detect damage and adapt its behavior autonomously. Resilience! ğŸ’ª

ğŸŒ The model proved its versatility by working across different robot configurations!

ğŸ”® This egocentric visual self-model could be the key to unlocking a new era of autonomous, adaptable, and resilient robots.

 **Yuhang Hu**, Boyuan Chen, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/xOv4nO4WWQQ)      ğŸ“„[PAPER](https://arxiv.org/pdf/2207.03386.pdf)


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/meta-sm.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Reconfigurable Robot Identification from Motion Data**

Can a robot autonomously understand and adapt to its physical form and functionalities through interaction with its environment? This question underscores the transition towards developing self-modeling robots without reliance on external sensory or pre-programmed knowledge about their structure. Here, we propose a meta- self-modeling that can deduce robot morphology through proprioception-the robotâ€™s internal sense of its bodyâ€™s position and movement. Our study introduces a 12-DoF reconfigurable legged robot, accompanied by a diverse dataset of 200k unique configurations, to systematically investigate the relationship between robotic motion and robot morphology.

**Yuhang Hu**, Yunzhe Wang, Ruibo Liu, Zhou Shen, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/WSaLwlacuy0?si=UalBN0N0Rx-N5w5l)      ğŸ“„[PAPER](https://arxiv.org/pdf/2403.10496.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023 @ Robot Learning Workshop</div><img src='images/knolling2.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**Enhancing Object Organization with Self-supervised Graspability Estimation**

A robotic system that integrates visual perception, a self-supervised graspability estimation model, knolling models, and robot arm controllers to efficiently organize an arbitrary number of objects on a table, even when they are closely clustered or stacked.

**Yuhang Hu**, Zhizhuo Zhang, Hod Lipson 

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=_pskiKonX38&t=12s)      ğŸ“„[PAPER](https://arxiv.org/pdf/2310.19226.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/eva.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Smile like you mean it: Driving animatronic robotic face with learned models**

Eva 2.0: A physical humanoid face robot with soft skin â• A vision-based self-supervised learning framework for facial mimicry.

Boyuan Chen, **Yuhang Hu**, Lianfeng Li, Sara Cummings, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/1vBLI-q04kM?si=gDofkitYstuisSRh)      ğŸ“„[PAPER](https://arxiv.org/pdf/2105.12724.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/hideandseek.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Visual perspective taking for opponent behavior modeling**

Humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict othersâ€™ plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). 

Boyuan Chen, **Yuhang Hu**, Robert Kwiatkowski, Shuran Song, Hod Lipson

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=gDtZoU3ayhg)      ğŸ“„[PAPER](https://arxiv.org/pdf/2105.05145.pdf)


