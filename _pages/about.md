---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---


<span class='anchor' id='about-me'></span>

# ğŸ”¥ News
<div class='paper-box'><div class='paper-box-image'><div><div class="badge">The New York Times</div><img src='images/new_york_times.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**â€˜Consciousnessâ€™ in Robots Was Once Taboo. Now Itâ€™s the Last Word.**

 ğŸ“„[The New York Times](https://www.nytimes.com/2023/01/06/science/robots-artificial-intelligence-consciousness.html)
 
</div>
</div>

# ğŸ“ Publications 

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Science Robotics </div><img src='images/emo_coexp.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**Human-robot facial coexpression**

Humanoid robots are capable of mimicking human expressions by perceiving human emotions and responding after the human has finished their expression. However, a delayed smile can feel artificial and disingenuous compared with a smile occurring simultaneously with a companionâ€™s smile. We trained our anthropomorphic facial robot named Emo to display an anticipatory expression to match its human companion. Emo is equipped with 26 motors and flexible silicone skin to provide precise control over its facial expressions. The robot was trained with a video dataset of humans making expressions. By observing subtle changes in a human face, the robot could predict an approaching smile 839 milliseconds before the human smiled and adjust its face to smile simultaneously.

**Yuhang Hu**, Boyuan Chen, Jiong Lin, Yunzhe Wang, Yingke Wang, Cameron Mehlman, Hod Lipson 

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=pWTTzR_wXuQ)      ğŸ“„[PAPER](https://www.science.org/doi/10.1126/scirobotics.adi4724)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Project in 2020</div><img src='images/bipedal.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Mini Bidedal Robot**

Design a legged robot starting from scratch, including CAD, electronics, embedding system, and deep reinforcement learning.

**Yuhang Hu**

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=Y0fBdpLf9ZI&t=4s)    

</div>
</div>


<div class='paper-box'><div class='paper-box-image'><div><div class="badge"> arxiv </div><img src='images/knolling.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Knolling Bot: Learning Robotic Object Arrangement from Tidy Demonstrations**

ğŸ¤– Attention busy people! Imagine you are very busy and don't have time to tell the robot where everything should be placed!ğŸ§¹ So, don't you think robots need to understand tidiness? ğŸ¤”

This paperğŸ“‘ shows how robots can learn the concept of ["knolling"](https://medium.com/@Lyst/your-favorite-insta-aesthetic-has-an-unexpected-history-a929e078d4ea) from tidy demonstrations, allowing them to automatically organize your table in a neat and efficient way. ğŸ§¹âœ¨


**Yuhang Hu**, Zhizhuo Zhang, Xinyue Zhu, Ruibo Liu, Philippe Wyder, Hod Lipson 

ğŸ¬[VIDEO](https://youtu.be/jCxykR4iP0I)      ğŸ“„[PAPER](https://arxiv.org/pdf/2310.04566.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/ego.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Egocentric Visual Self-Modeling for Autonomous Robot Dynamics Prediction and Adaptation**

We developed an approach that allows robots to learn their own dynamics using only a first-person camera view, without any prior knowledge! ğŸ¥ğŸ’¡

ğŸ¦¿ Tested on a 12-DoF robot, the self-supervised model showcased the capabilities of basic locomotion tasks.

ğŸ”§ The robot can detect damage and adapt its behavior autonomously. Resilience! ğŸ’ª

ğŸŒ The model proved its versatility by working across different robot configurations!

ğŸ”® This egocentric visual self-model could be the key to unlocking a new era of autonomous, adaptable, and resilient robots.

 **Yuhang Hu**, Boyuan Chen, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/xOv4nO4WWQQ)      ğŸ“„[PAPER](https://arxiv.org/pdf/2207.03386.pdf)


</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">arxiv</div><img src='images/meta-sm.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Reconfigurable Robot Identification from Motion Data**

Can a robot autonomously understand and adapt to its physical form and functionalities through interaction with its environment? This question underscores the transition towards developing self-modeling robots without reliance on external sensory or pre-programmed knowledge about their structure. Here, we propose a meta- self-modeling that can deduce robot morphology through proprioception-the robotâ€™s internal sense of its bodyâ€™s position and movement. Our study introduces a 12-DoF reconfigurable legged robot, accompanied by a diverse dataset of 200k unique configurations, to systematically investigate the relationship between robotic motion and robot morphology.

**Yuhang Hu**, Yunzhe Wang, Ruibo Liu, Zhou Shen, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/WSaLwlacuy0?si=UalBN0N0Rx-N5w5l)      ğŸ“„[PAPER](https://arxiv.org/pdf/2403.10496.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">NeurIPS 2023 @ Robot Learning Workshop</div><img src='images/knolling2.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">
  
**Enhancing Object Organization with Self-supervised Graspability Estimation**

A robotic system that integrates visual perception, a self-supervised graspability estimation model, knolling models, and robot arm controllers to efficiently organize an arbitrary number of objects on a table, even when they are closely clustered or stacked.

**Yuhang Hu**, Zhizhuo Zhang, Hod Lipson 

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=_pskiKonX38&t=12s)      ğŸ“„[PAPER](https://arxiv.org/pdf/2310.19226.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/eva.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Smile like you mean it: Driving animatronic robotic face with learned models**

Eva 2.0: A physical humanoid face robot with soft skin â• A vision-based self-supervised learning framework for facial mimicry.

Boyuan Chen, **Yuhang Hu**, Lianfeng Li, Sara Cummings, Hod Lipson

ğŸ¬[VIDEO](https://youtu.be/1vBLI-q04kM?si=gDofkitYstuisSRh)      ğŸ“„[PAPER](https://arxiv.org/pdf/2105.12724.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">ICRA 2021</div><img src='images/hideandseek.gif' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Visual perspective taking for opponent behavior modeling**

Humans learn at a young age to infer what others see and cannot see from a different point-of-view, and learn to predict othersâ€™ plans and behaviors. These abilities have been mostly lacking in robots, sometimes making them appear awkward and socially inept. Here we propose an end-to-end long-term visual prediction framework for robots to begin to acquire both these critical cognitive skills, known as Visual Perspective Taking (VPT) and Theory of Behavior (TOB). 

Boyuan Chen, **Yuhang Hu**, Robert Kwiatkowski, Shuran Song, Hod Lipson

ğŸ¬[VIDEO](https://www.youtube.com/watch?v=gDtZoU3ayhg)      ğŸ“„[PAPER](https://arxiv.org/pdf/2105.05145.pdf)

</div>
</div>

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">Nature Nanotechnology</div><img src='images/nature_nano.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

**Multiplexed reverse-transcriptase quantitative polymerase chain reaction using plasmonic nanoparticles for point-of-care COVID-19 diagnosis**

Quantitative polymerase chain reaction (qPCR) offers the capabilities of real-time monitoring of amplified products, fast detection, and quantitation of infectious units, but poses technical hurdles for point-of-care miniaturization compared with end-point polymerase chain reaction. Here we demonstrate plasmonic thermocycling, in which rapid heating of the solution is achieved via infrared excitation of nanoparticles, successfully performing reverse-transcriptase qPCR (RT-qPCR) in a reaction vessel containing polymerase chain reaction chemistry, fluorescent probes and plasmonic nanoparticles. The method could rapidly detect SARS-CoV-2 RNA from human saliva and nasal specimens with 100% sensitivity and 100% specificity, as well as two distinct SARS-CoV-2 variants.

Nicole R Blumenfeld, Michael Anne E Bolene, Martin Jaspan, Abigail G Ayers, Sabin Zarrandikoetxea, Juliet Freudman, Nikhil Shah, Angela M Tolwani, **Yuhang Hu**, Terry L Chern, James Rogot, Vira Behnam, Aditya Sekhar, Xinyi Liu, Bulent Onalir, Robert Kasumi, Abdoulaye Sanogo, Kelia Human, Kasey Murakami, Goutham S Totapally, Mark Fasciano, Samuel K Sia

ğŸ“„[PAPER](https://www.nature.com/articles/s41565-022-01175-4)


</div>
</div>




